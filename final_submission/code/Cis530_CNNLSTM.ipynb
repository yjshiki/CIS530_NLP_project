{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Cis530_CNNLSTM.ipynb","provenance":[{"file_id":"1YqG3V40BQMz8ft6NXceP5DyUV-UWci7T","timestamp":1587668726836}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"CNS4OZXxMZ-A","outputId":"69ed9768-ae86-4f96-d98d-173606cb70d8","executionInfo":{"status":"ok","timestamp":1588439805667,"user_tz":240,"elapsed":177088,"user":{"displayName":"Shiping Yi","photoUrl":"","userId":"02206975424748876812"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["#Download and unzip files\n","!pip3 install scikit-learn\n","!sudo apt-get install unzip\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torch torchvision\n","!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","\n","import torch\n","device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","unzip is already the newest version (6.0-21ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Collecting torch==1.0.1\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n","\u001b[K     |████████████████████████████████| 614.8MB 29kB/s \n","\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.5.0+cu101\n","    Uninstalling torch-1.5.0+cu101:\n","      Successfully uninstalled torch-1.5.0+cu101\n","Successfully installed torch-1.0.1\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.3)\n","--2020-05-02 17:15:59--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.93.229\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.93.229|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  36.2MB/s    in 44s     \n","\n","2020-05-02 17:16:43 (35.4 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qIJxbqMDNTTl","outputId":"faf76d96-b108-419b-8cc5-5c0a45d36a3b","executionInfo":{"status":"ok","timestamp":1588439810402,"user_tz":240,"elapsed":4714,"user":{"displayName":"Shiping Yi","photoUrl":"","userId":"02206975424748876812"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Verfiy file download\n","#models.py\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset\n","import itertools\n","import csv\n","import pandas as pd\n","import spacy\n","from torchtext.vocab import GloVe\n","import re\n","from torchtext import data\n","import pickle\n","import torch.optim as optim\n","import gensim\n","import torchtext.vocab as vocab\n","from tqdm import tqdm_notebook\n","#Verify CUDA acceleration should print cuda:0\n","print(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"py5FZ6IKWdFd","colab_type":"code","outputId":"de782859-688f-42a9-b5bf-5a7401d7d4e8","executionInfo":{"status":"ok","timestamp":1588439929256,"user_tz":240,"elapsed":123550,"user":{"displayName":"Shiping Yi","photoUrl":"","userId":"02206975424748876812"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["from torchtext.data import Field\n","import spacy\n","from gensim.models import KeyedVectors\n","\n","# Train Word Embeddings and save\n"," \n","EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n","model_em = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n","weights = torch.FloatTensor(model_em.vectors)\n","t_embedding = nn.Embedding.from_pretrained(weights)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NbOusBLKPsrx","colab":{}},"source":["# NLP = spacy.load('en')\n","MAX_CHARS = 20000\n","MAX_VOCAB_SIZE = 25000\n","BATCH_SIZE = 64\n","comment = data.Field(\n","    tokenize='spacy'\n",")\n","label = data.LabelField()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","def load_data(train_file, val_file, test_file):\n","\n","    train = data.TabularDataset(\n","        path=train_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    val = data.TabularDataset(\n","        path=val_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    test = data.TabularDataset(\n","        path=test_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    \n","    comment.build_vocab(train,\n","                        max_size=MAX_VOCAB_SIZE,\n","                        min_freq=5,\n","                        #vectors=GloVe(name='6B', dim=300),\n","                        #unk_init=torch.Tensor.normal_\n","                        )\n","          \n","    label.build_vocab(train)\n","    print(label.vocab.stoi)\n","\n","    BATCH_SIZE = 64\n","\n","    train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n","        (train, val, test),\n","        batch_size=BATCH_SIZE,\n","        device=device,\n","        sort=False)\n","    # pickle.dump(train, open(\"train_iter.p\", \"wb\"))\n","    # pickle.dump(val, open(\"val_iter.p\", \"wb\"))\n","    # pickle.dump(test, open(\"test_iter.p\", \"wb\"))\n","    return train_iterator, val_iterator, test_iterator\n","\n","\n","class LSTMCNN(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n","                 dropout, pad_idx, batch_size):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.convs = nn.ModuleList([\n","            nn.Conv2d(in_channels=1,\n","                      out_channels=n_filters,\n","                      kernel_size=(fs, embedding_dim))\n","            for fs in filter_sizes\n","        ])\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)       \n","        self.dropout = nn.Dropout(dropout)\n","        self.lstm2 =nn.LSTM(\n","            input_size=embedding_dim,\n","            hidden_size=embedding_dim,\n","            num_layers=2,\n","            dropout =0.5\n","           )\n","\n","  def forward(self, text):\n","      # LSTM\n","      text = text.permute(1, 0)\n","      embedded = self.embedding(text)\n","      embedded.permute(2,1,0) #[64, 33, 300]\n","      embedded, __ = self.lstm2(embedded) # (seq_len, batch, num_directions * hidden_size)\n","      # CNN\n","      embedded = embedded.unsqueeze(1)# [64, 1, 33, 300]\n","      conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","      pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","      cat = self.dropout(torch.cat(pooled, dim=1))\n","      return self.fc(cat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"92yvSRry-9mV","colab_type":"code","colab":{}},"source":["# combined model\n","class CNNLSTM(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n","                 dropout, pad_idx, batch_size):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.convs = nn.ModuleList([\n","            nn.Conv2d(in_channels=1,\n","                      out_channels=n_filters,\n","                      kernel_size=(fs, embedding_dim))\n","            for fs in filter_sizes\n","        ])\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)       \n","        self.dropout = nn.Dropout(dropout)\n","        self.lstm = nn.LSTM(\n","            embedding_dim,\n","            hidden_size=64,\n","            num_layers=1)\n","        \n","        self.hidden2label1 = nn.Linear(64,64//2)\n","        self.hidden2label2 = nn.Linear(64//2,output_dim)\n","\n","  def forward(self, text):\n","      embedded = self.embedding(text)  \n","      embedded = self.dropout(embedded)           \n","      embedded = embedded.permute(1,0,2) \n","      embedded = embedded.unsqueeze(1)      \n","      conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]            \n","      cnn_out = torch.cat(conved,0)\n","      cnn_out = torch.transpose(cnn_out,1,2)\n","      \n","      #LSTM layer\n","      # [64, 28, 300]\n","      lstm_out, hidden1 = self.lstm(cnn_out)\n","      # [64, 28, 64]\n","      lstm_out = torch.transpose(lstm_out,1,2)\n","      # 64,64,28\n","      lstm_out = F.max_pool1d(lstm_out,lstm_out.size(2)).squeeze(2)\n","      #[64,64]\n","\n","      # linear\n","      combined = self.hidden2label1(lstm_out)\n","      combined = self.hidden2label2(combined)\n","      return combined\n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAeLs1n5RKEf","colab_type":"code","colab":{}},"source":["############################################################\n","# Reference Code\n","############################################################\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n","    correct = max_preds.squeeze(1).eq(y)\n","    return correct.sum() / torch.FloatTensor([y.shape[0]]), max_preds, correct\n","\n","\n","def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in iterator:\n","        preds=[]\n","        trues=[]\n","        optimizer.zero_grad()        \n","        predictions = model(batch.text)\n","        loss = criterion(predictions, batch.label)\n","        acc,max_preds,correct = categorical_accuracy(predictions, batch.label)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        # f1_score\n","        preds += predictions.squeeze(1).argmax(dim=1).tolist()\n","        trues += batch.label.tolist()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), preds, trues\n","\n","\n","def evaluate(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    preds=[]\n","    trues=[]\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.text)\n","            loss = criterion(predictions, batch.label)\n","            acc, max_pred, correct = categorical_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            # f1_score\n","            preds += predictions.squeeze(1).argmax(dim=1).tolist()\n","            trues += batch.label.tolist()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), preds, trues\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBVpkxmrRdFs","colab_type":"code","colab":{}},"source":["  \n","  train_iterator, val_iterator, test_iterator = load_data('train.csv', 'val.csv','test.csv')\n","  INPUT_DIM = len(comment.vocab)#comment.vocab\n","  EMBEDDING_DIM = 300\n","  N_FILTERS = 300\n","  FILTER_SIZES = [5]\n","  OUTPUT_DIM = len(label.vocab)\n","  DROPOUT = 0.5\n","  PAD_IDX = comment.vocab.stoi[comment.pad_token]\n","  N_EPOCHS = 5\n","\n","  model = LSTMCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX, BATCH_SIZE)\n","  model.embedding = t_embedding\n","  #UNK_IDX = comment.vocab.stoi[comment.unk_token]\n","  #model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","  #model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n"," \n","  optimizer = optim.Adadelta(model.parameters(), weight_decay=1e-5) # apply l2-reg\n","  criterion = nn.CrossEntropyLoss()\n","  #model.to(torch.device('cuda'))\n","  model = model.to(device)\n","  criterion = criterion.to(device)\n","  best_valid_loss = float('inf')\n","  train_preds=[]\n","  train_trues=[]\n","  val_preds =[]\n","  val_trues =[]\n","  for epoch in range(N_EPOCHS):\n","\n","      start_time = time.time()\n","      train_loss, train_acc, train_pred, train_true = train(model, train_iterator, optimizer, criterion)\n","      valid_loss, valid_acc, val_pred, val_true = evaluate(model, val_iterator, criterion)\n","      val_preds.extend(val_pred)\n","      val_trues.extend(val_true)\n","      train_preds.extend(train_pred)\n","      train_trues.extend(train_true)\n","\n","      end_time = time.time()\n","\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","      if valid_loss < best_valid_loss:\n","          best_valid_loss = valid_loss\n","          torch.save(model.state_dict(), 'baseline-model.pt')\n","\n","      print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n","      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDpaGtcVFpgP","colab_type":"code","colab":{}},"source":["\n","test_loss, test_acc, test_preds, test_trues = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n","  \n","# f1 score\n","from sklearn.metrics import f1_score,recall_score, precision_score\n","#print(f1_score(preds, trues, average='weighted'))\n","# training \n","print(f1_score(train_trues, train_preds, average='macro'))\n","print(precision_score(train_trues,train_preds,average='macro'))\n","print(recall_score(train_trues, train_preds,average='macro'))\n","\n","# validation\n","print(f1_score(val_trues, val_preds, average='macro'))\n","print(precision_score(val_trues,val_preds,average='macro'))\n","print(recall_score(val_trues, val_preds,average='macro'))\n","\n","#test \n","print(f1_score(test_trues, test_preds, average='macro'))\n","print(precision_score(test_trues,test_preds,average='macro'))\n","print(recall_score(test_trues, test_preds,average='macro'))\n","\n","\n","'''\n","0.8812620211468666 f\n","0.8854992768977231\n","0.8774410774410774\n","0.9104255886331983 f\n","0.9304217405792423\n","0.8945494308819626\n","0.9200114337537629 f\n","0.9372097195318804\n","0.9060100274986641\n","'''"],"execution_count":0,"outputs":[]}]}