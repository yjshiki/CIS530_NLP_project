{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Cis530_extension1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"CNS4OZXxMZ-A","outputId":"3c772628-0933-40ca-8801-f4bb83044d98","colab":{"base_uri":"https://localhost:8080/","height":578},"executionInfo":{"status":"ok","timestamp":1588458559560,"user_tz":240,"elapsed":159173,"user":{"displayName":"Yujie Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuuq-1Dy2syssHKl4i4kWXNAqAL1WnGvcbbNyG=s64","userId":"11051088488429043450"}}},"source":["#Download and unzip files\n","!pip3 install scikit-learn\n","!sudo apt-get install unzip\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torch torchvision\n","!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","unzip is already the newest version (6.0-21ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Collecting torch==1.0.1\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n","\u001b[K     |████████████████████████████████| 614.8MB 28kB/s \n","\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.5.0+cu101\n","    Uninstalling torch-1.5.0+cu101:\n","      Successfully uninstalled torch-1.5.0+cu101\n","Successfully installed torch-1.0.1\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","--2020-05-02 22:28:45--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.29.246\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.29.246|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  35.5MB/s    in 31s     \n","\n","2020-05-02 22:29:16 (50.6 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qIJxbqMDNTTl","outputId":"781e897e-5de0-4364-b859-cb8759f2fb33","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1588458570558,"user_tz":240,"elapsed":3291,"user":{"displayName":"Yujie Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuuq-1Dy2syssHKl4i4kWXNAqAL1WnGvcbbNyG=s64","userId":"11051088488429043450"}}},"source":["#Verfiy file download\n","#models.py\n","import torch\n","device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset\n","import itertools\n","import csv\n","import pandas as pd\n","import spacy\n","from torchtext.vocab import GloVe\n","import re\n","from torchtext import data\n","import pickle\n","import torch.optim as optim\n","import gensim\n","import torchtext.vocab as vocab\n","from tqdm import tqdm_notebook\n","#Verify CUDA acceleration should print cuda:0\n","print(device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1eKT4SoNP3lH","outputId":"42c53e1d-539d-4478-cbd0-0dc8d6df69f3","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1588458605790,"user_tz":240,"elapsed":28696,"user":{"displayName":"Yujie Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuuq-1Dy2syssHKl4i4kWXNAqAL1WnGvcbbNyG=s64","userId":"11051088488429043450"}}},"source":["# Mount your google drive. \n","# Use this to save your PyTorch model for submission\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","#Test drive access. \n","#You should have a test.txt under a new folder cis530_hw6 in your Google drive\n","!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","gdrive\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"py5FZ6IKWdFd","colab_type":"code","outputId":"8a98155b-0cfc-4b06-8f33-97114917cd37","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1588458701824,"user_tz":240,"elapsed":88449,"user":{"displayName":"Yujie Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuuq-1Dy2syssHKl4i4kWXNAqAL1WnGvcbbNyG=s64","userId":"11051088488429043450"}}},"source":["from torchtext.data import Field\n","import spacy\n","from gensim.models import KeyedVectors\n","\n","# Train Word Embeddings and save\n"," \n","EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n","model_em = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n","weights = torch.FloatTensor(model_em.vectors)\n","t_embedding = nn.Embedding.from_pretrained(weights)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NbOusBLKPsrx","colab":{}},"source":["# NLP = spacy.load('en')\n","MAX_CHARS = 20000\n","MAX_VOCAB_SIZE = 25000\n","comment = data.Field(\n","    tokenize='spacy'\n",")\n","label = data.LabelField()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","def load_data(train_file, val_file, test_file):\n","\n","    train = data.TabularDataset(\n","        path=train_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    val = data.TabularDataset(\n","        path=val_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    test = data.TabularDataset(\n","        path=test_file, format='csv', skip_header=True,\n","        fields=[\n","            ('label', label),\n","            ('text', comment),\n","        ])\n","    \n","    comment.build_vocab(train, vectors=GloVe(name='6B', dim=100),\n","                        max_size=MAX_VOCAB_SIZE,\n","                        min_freq=5,\n","                        unk_init=torch.Tensor.normal_\n","                        )#\n","          \n","    label.build_vocab(train)\n","    print(label.vocab.stoi)\n","\n","    BATCH_SIZE = 64\n","\n","    train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n","        (train, val, test),\n","        batch_size=BATCH_SIZE,\n","        device=device,\n","        sort=False)\n","    # pickle.dump(train, open(\"train_iter.p\", \"wb\"))\n","    # pickle.dump(val, open(\"val_iter.p\", \"wb\"))\n","    # pickle.dump(test, open(\"test_iter.p\", \"wb\"))\n","    return train_iterator, val_iterator, test_iterator\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n","                 dropout, pad_idx):\n","        super().__init__()\n","\n","        self.embedding =  nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.convs = nn.ModuleList([\n","            nn.Conv2d(in_channels=1,\n","                      out_channels=n_filters,\n","                      kernel_size=(fs, embedding_dim))\n","            for fs in filter_sizes\n","        ])\n","\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","\n","        text = text.permute(1, 0)\n","        embedded = self.embedding(text)\n","        embedded = embedded.unsqueeze(1)\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        cat = self.dropout(torch.cat(pooled, dim=1))\n","        return self.fc(cat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAeLs1n5RKEf","colab_type":"code","colab":{}},"source":["############################################################\n","# Reference Code\n","############################################################\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n","    correct = max_preds.squeeze(1).eq(y)\n","    return correct.sum() / torch.FloatTensor([y.shape[0]]), max_preds, correct\n","\n","\n","def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in iterator:\n","        preds=[]\n","        trues=[]\n","        optimizer.zero_grad()\n","        predictions = model(batch.text)\n","        loss = criterion(predictions, batch.label)\n","        acc,max_preds,correct = categorical_accuracy(predictions, batch.label)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        # f1_score\n","        preds += predictions.squeeze(1).argmax(dim=1).tolist()\n","        trues += batch.label.tolist()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), preds, trues\n","\n","\n","def evaluate(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    preds=[]\n","    trues=[]\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.text)\n","            loss = criterion(predictions, batch.label)\n","            acc, max_pred, correct = categorical_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            # f1_score\n","            preds += predictions.squeeze(1).argmax(dim=1).tolist()\n","            trues += batch.label.tolist()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), preds, trues\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LPd8Lkh38j2","colab_type":"code","outputId":"552d0a80-599f-40bc-9af9-05232b73265e","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"error","timestamp":1588458780259,"user_tz":240,"elapsed":12194,"user":{"displayName":"Yujie Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuuq-1Dy2syssHKl4i4kWXNAqAL1WnGvcbbNyG=s64","userId":"11051088488429043450"}}},"source":["# import os\n","# os.listdir(\"./gdrive/My Drive/530/\")"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-fcdea8b3c22a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gdrive/My Drive/530/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './gdrive/My Drive/530/'"]}]},{"cell_type":"code","metadata":{"id":"g9jMHsnZhZN2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwKbTxZxhZZc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjGVaLOzhZcU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDtRyCFZhZfI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBVpkxmrRdFs","colab_type":"code","outputId":"7135752c-ce74-478d-e738-1c97a02aef7f","colab":{"base_uri":"https://localhost:8080/","height":694}},"source":["  \n","  train_iterator, val_iterator, test_iterator = load_data('./gdrive/My Drive/530/train.csv', './gdrive/My Drive/530/val.csv', \n","                                                          './gdrive/My Drive/530/test.csv')\n","  INPUT_DIM = len(comment.vocab)# comment.vocab\n","  EMBEDDING_DIM = 300\n","  N_FILTERS = 300\n","  FILTER_SIZES = [3,4,5]\n","  #3,4,5\n","  OUTPUT_DIM = len(label.vocab)\n","  DROPOUT = 0.5\n","  PAD_IDX = comment.vocab.stoi[comment.pad_token]\n","  N_EPOCHS = 5\n","\n","  model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n","  model.embedding = t_embedding\n","  #UNK_IDX = comment.vocab.stoi[comment.unk_token]\n","\n","  #model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","  #model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n"," \n","  optimizer = optim.Adadelta(model.parameters(), weight_decay=1e-5) # apply l2-reg\n","  criterion = nn.CrossEntropyLoss()\n","  model = model.to(device)\n","  criterion = criterion.to(device)\n","  best_valid_loss = float('inf')\n","  train_preds=[]\n","  train_trues=[]\n","  val_preds =[]\n","  val_trues =[]\n","  for epoch in range(N_EPOCHS):\n","\n","      start_time = time.time()\n","      train_loss, train_acc, train_pred, train_true = train(model, train_iterator, optimizer, criterion)\n","      valid_loss, valid_acc, val_pred, val_true = evaluate(model, val_iterator, criterion)\n","      val_preds.extend(val_pred)\n","      val_trues.extend(val_true)\n","      train_preds.extend(train_pred)\n","      train_trues.extend(train_true)\n","\n","      end_time = time.time()\n","\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","      if valid_loss < best_valid_loss:\n","          best_valid_loss = valid_loss\n","          torch.save(model.state_dict(), 'baseline-model.pt')\n","\n","      print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n","      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                           \n","100%|█████████▉| 399886/400000 [00:13<00:00, 27221.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["defaultdict(<function _default_unk_index at 0x7fbc1ca1abf8>, {'0': 0, '1': 1, '-1': 2})\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|█████████▉| 399886/400000 [00:30<00:00, 27221.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.337 | Train Acc: 88.05%\n","\t Val. Loss: 0.198 |  Val. Acc: 93.52%\n","Epoch: 02 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.228 | Train Acc: 92.51%\n","\t Val. Loss: 0.174 |  Val. Acc: 94.46%\n","Epoch: 03 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.199 | Train Acc: 93.59%\n","\t Val. Loss: 0.166 |  Val. Acc: 94.82%\n","Epoch: 04 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.183 | Train Acc: 94.16%\n","\t Val. Loss: 0.161 |  Val. Acc: 95.14%\n","Epoch: 05 | Epoch Time: 1m 12s\n","\tTrain Loss: 0.171 | Train Acc: 94.55%\n","\t Val. Loss: 0.159 |  Val. Acc: 95.20%\n","Epoch: 06 | Epoch Time: 1m 12s\n","\tTrain Loss: 0.163 | Train Acc: 94.83%\n","\t Val. Loss: 0.157 |  Val. Acc: 95.35%\n","Epoch: 07 | Epoch Time: 1m 12s\n","\tTrain Loss: 0.156 | Train Acc: 95.04%\n","\t Val. Loss: 0.161 |  Val. Acc: 95.35%\n","Epoch: 08 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.151 | Train Acc: 95.22%\n","\t Val. Loss: 0.155 |  Val. Acc: 95.58%\n","Epoch: 09 | Epoch Time: 1m 12s\n","\tTrain Loss: 0.147 | Train Acc: 95.34%\n","\t Val. Loss: 0.149 |  Val. Acc: 95.65%\n","Epoch: 10 | Epoch Time: 1m 12s\n","\tTrain Loss: 0.145 | Train Acc: 95.41%\n","\t Val. Loss: 0.159 |  Val. Acc: 95.46%\n","Epoch: 11 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.142 | Train Acc: 95.49%\n","\t Val. Loss: 0.149 |  Val. Acc: 95.67%\n","Epoch: 12 | Epoch Time: 1m 11s\n","\tTrain Loss: 0.140 | Train Acc: 95.55%\n","\t Val. Loss: 0.150 |  Val. Acc: 95.74%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NDpaGtcVFpgP","colab_type":"code","outputId":"b969ce52-91bb-42d0-8a0d-2567d9f97e99","colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["\n","test_loss, test_acc, test_preds, test_trues = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n","  \n","# f1 score\n","from sklearn.metrics import f1_score,recall_score, precision_score\n","#print(f1_score(preds, trues, average='weighted'))\n","# training \n","print(f1_score(train_trues, train_preds, average='macro'))\n","print(precision_score(train_trues,train_preds,average='macro'))\n","print(recall_score(train_trues, train_preds,average='macro'))\n","\n","# validation\n","print(f1_score(val_trues, val_preds, average='macro'))\n","print(precision_score(val_trues,val_preds,average='macro'))\n","print(recall_score(val_trues, val_preds,average='macro'))\n","\n","#test \n","print(f1_score(test_trues, test_preds, average='macro'))\n","print(precision_score(test_trues,test_preds,average='macro'))\n","print(recall_score(test_trues, test_preds,average='macro'))\n","\n","\n","'''\n","0.8812620211468666\n","0.8854992768977231\n","0.8774410774410774\n","0.9104255886331983\n","0.9304217405792423\n","0.8945494308819626\n","0.9200114337537629\n","0.9372097195318804\n","0.9060100274986641\n","\n","300 5,5\n","0.9357373530580609\n","\n","300 3,4,5\n","0.94\n","'''"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Loss: 0.147 | Test Acc: 95.81%\n","0.9254674670554447\n","0.9312078929066577\n","0.9201562002274734\n","0.9373028159422666\n","0.9475678797007289\n","0.9282059324979764\n","0.9454422659326581\n","0.951540857305799\n","0.9397809398751455\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\n0.8812620211468666\\n0.8854992768977231\\n0.8774410774410774\\n0.9104255886331983\\n0.9304217405792423\\n0.8945494308819626\\n0.9200114337537629\\n0.9372097195318804\\n0.9060100274986641\\n\\n300 5,5\\n0.9357373530580609\\n\\n300 3,4,5\\n0.94\\n'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"G7jb7E-izm5G","colab_type":"code","outputId":"4febb04c-2b9f-476c-fb99-c1b94d258b68","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["print(len(test_preds))\n","print(len(test_trues))\n","for i in range(100):\n","  if test_preds[i]!=test_trues[i]:\n","    print(i,test_preds[i],test_trues[i])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["235992\n","235992\n","0 1 0\n","6 2 1\n","40 2 1\n","49 0 2\n","69 0 2\n","83 0 1\n","84 0 2\n","98 0 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fg17-QW7jQbl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}